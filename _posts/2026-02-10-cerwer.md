---
layout: post
title: "[Speech Metrics] CER & WER Explained (and How We Benchmark STT Quality)"
categories: [Speech, ASR, Metrics]
tags: [cer, wer, asr, stt, evaluation, benchmarking]
pin: true
---

When working with **Speech-to-Text (STT)** systems, the most common way to quantify transcription quality is through **CER (Character Error Rate)** and **WER (Word Error Rate)**.  
This post explains what CER/WER mean, when each metric is useful, and how we benchmark them in practice (public datasets + internal annotated sets).

---

## âœ… Goal

By the end of this post, youâ€™ll be able to:

- Understand what **CER** and **WER** measure
- Know how they are computed (intuition + formula)
- Decide **when to use CER vs WER**
- Set up a practical benchmarking strategy (public + internal)

---

## ğŸ§  What Are CER and WER?

Both CER and WER are based on **edit distance** (Levenshtein distance).  
They measure how many edits are required to transform the STT output into the ground-truth transcript.

Edits include:

- **Substitution (S)**: wrong character/word
- **Deletion (D)**: missing character/word
- **Insertion (I)**: extra character/word

---

## ğŸ”¤ CER (Character Error Rate)

**CER** evaluates errors at the **character level**.

### âœ… Formula

\[
CER = \frac{S + D + I}{N}
\]

- **S** = substitutions (characters)
- **D** = deletions (characters)
- **I** = insertions (characters)
- **N** = number of characters in the reference (ground truth)

### ğŸŸ¢ When CER is especially useful

- Languages where spacing/word segmentation is tricky (e.g., Korean)
- You want sensitivity to **small spelling/particle changes**
- You care about **fine-grained** transcription quality

---

## ğŸ§¾ WER (Word Error Rate)

**WER** evaluates errors at the **word level**.

### âœ… Formula

\[
WER = \frac{S + D + I}{N}
\]

Same structure as CER, but counts are based on **words**, and **N** is the number of words in the reference.

### ğŸŸ¢ When WER is especially useful

- English-like languages where words are clearly separated by spaces
- You want a metric aligned with â€œhow readable the sentence isâ€
- Your downstream system treats STT output as word tokens

---

## ğŸ†š CER vs WER: Which One Should I Use?

Hereâ€™s a practical rule of thumb:

- If your language/tokenization is **ambiguous** â†’ use **CER**
- If your language is **space-delimited** and word-based â†’ use **WER**
- For Korean STT evaluation, many teams report **both**, but often rely more on **CER**.

---

## ğŸ§ª Practical Benchmarking Setup (How We Track STT Quality)

In our STT evaluation workflow, we typically calculate CER/WER using **two categories of benchmarks**:

### 1ï¸âƒ£ Public (External) ASR Datasets

We select several public Korean ASR datasets (typically 3â€“4) and compute CER/WER on them.

**Why this matters:**
- Provides a standardized point of comparison
- Helps detect regressions that generalize beyond internal call data

**Limitations:**
- Public data may not reflect your real production domain (e.g., insurance calls, TMR scripts, noisy channels)

---

### 2ï¸âƒ£ Internal (Manual) Benchmark with In-house Annotations

We also maintain an internal benchmark set annotated by our teams (e.g., London team + partner teams).  
This is especially important when:

- Domain language is specific (insurance products, consent scripts, policy terms)
- Audio conditions differ (telephony, latency artifacts, background noise)
- The STT model is optimized for production-style calls

We maintain internal benchmark subsets by time windows, such as:

- **Subset 1:** Julyâ€“September  
- **Subset 2:** Octoberâ€“November  
- **Subset 3:** Decemberâ€“January  

**Why split by time?**
- Audio distribution and scenarios can drift over time (new scripts, new call patterns, seasonal effects, model updates)
- It helps us compare model performance across consistent â€œerasâ€ of data

---

## ğŸ“Œ Notes That Make Your Metrics More Trustworthy

A few practical tips that improve interpretability:

- Always define **normalization rules** (numbers, punctuation, spacing, special tokens)
- Ensure the same **tokenization policy** is used consistently across time
- Report:
  - Average CER/WER
  - And if possible, breakdowns by scenario (e.g., consent vs general ìƒë‹´)

---

## ğŸ“ Conclusion

CER and WER are simple but powerful metrics to track STT quality.  
In practice, the best evaluation approach is to combine:

- **Public datasets** â†’ general robustness check  
- **Internal annotated benchmarks** â†’ domain realism + production relevance  

If you want, I can also share a Python snippet for computing CER/WER consistently (including Korean normalization rules) and a template for reporting results across benchmark subsets.

---