---
layout: post
title: "[Comp] Click-through rate(CTR) prediction"
date: 2020-02-16 18:13:00 +0800
categories: [Competition, CTR prediction]
tags: [IGAWorks, CTR prediction]
toc : true
comments: true
---
![Desktop View](/assets/img/sample/[comp]igaworks.png)

# 대회소개
[클릭률](https://support.google.com/google-ads/answer/2615875?hl=en) 예측은 입찰가 결정에 지대한 영향을 끼치고, 이는 곧 매출과 직결되어 있다. 따라서 사용자와 방문 페이지에 따른 광고 클릭 확률을 추정하는 것은 매우 중요한 문제이다. 이번 예측 대회에서는 특정 고객이 특정 사이트에 접속했을 때 특정 광고를 클릭할 확률을 예측해보는 대회이다. 연세대학교 [**데이터사이언스랩**](https://yonseidslab.github.io/projects) 학회원인 [민형규](https://hgmin1159.github.io/2020/02/16/IGAworks-%ED%81%B4%EB%A6%AD%EC%9C%A8-%EC%98%88%EC%B8%A1-%EB%8C%80%ED%9A%8C.html), [권다희](https://daheekwon.github.io/)와 함께 대회에 참가했으나, 결론적으로 우리팀은 본선을 진출하지 못하였다. 짧은 준비기간임에도 불구하고 팀워크가 잘 맞았고 새로이 배운 점도 많았기에 아쉬움이 컸던 대회였다.

>우리는 이번 공동 포스팅을 통해 배운 것을 정리하고 부족한 부분을 점검하여 다음 대회를 위한 준비를 하기로 했다.

***  

# 키포인트
공모전은 두 가지로 데이터 셋으로 이루어져 있다. Train Data는 모바일 이용객이 광고를 접하는 상황에 대한 정보를, Audience_Profile은 모바일 유저들에 대한 일련의 정보를 포함하고 있다. 모든 명목형 자료들이 철저히 암호화되어있었기 때문에 외부 데이터를 사용하는 것은 불가능했다. 이 데이터 셋으로 예측모델을 짜기 위해서는 몇 가지 핵심 문제들을 해결해주었어야 했다.

## 첫 번째: 대용량 처리
Train 데이터의 경우 550만개의 자료가 24개의 변수를 가지고 있었다. 심지어 그 중 20개가 “WD23SLDC” 따위의 암호화 변수였으므로 데이터의 크기는 1.5Gb에 달했다. Audience_Profile은 1000만개 이상의 자료가 저장되어 있었고 파일크기는 9Gb에 달했다. 두 자료 모두 로컬에서는 돌리기가 힘든 크기였다. 라벨인코딩 등을 이용하여 축소시키고, 불필요한 자료를 버림으로써 로컬에서 핸들링은 가능하였으나, 실제 알고리즘을 돌릴 때는 메모리가 턱없이 부족했다.

따라서 우리는 구글 Colab에서 지원하는 25Gb의 메모리자원을 사용하였으나, 차후에 언급할 알고리즘들은 이 정도로도 부족해 GCP의 n1-Highmem-32(메모리 320Gb)를 사용했다. 이를 위한 GCP 사용법은 아래에 링크로 달아놓았다.


## 두 번째, 자연어 처리
Audience 데이터는 사용자가 설치한 어플리케이션 항목과 IGAWorks에서 측정한 사용자의 카테고리 정보 등을 변수로 갖고 있다. 해당 변수들은 껄끄러운 자연어 형태로 큰 용량을 차지하고 있었으며, 자료간에 겹치는 부분 또한 매우 적었다. 따라서 정확한 전처리 과정으로 데이터 분석의 기반을 탄탄히 다질 필요가 있었다. 처리 후에는 여러 분석을 통해 새로운 변수를 만들어냄으로써, 예측의 정확도를 높여야할 필요가 있었다.  

Audience 데이터를 바탕으로 비지도학습 모델을 적용하여 유사한 성질의 사용자끼리 그룹화하는 과정을 아래에 링크로 달아놓았다.


## 세 번째, 차원축소
Train 데이터의 20개의 변수를 모두 더미화하면 3만개에 달했다. 이러한 데이터를 컴퓨터에서 다룰 수 없을뿐더러 오버피팅 문제가 발생하기 때문에, Train 데이터의 주요화두는 차원축소였다. 따라서 처음에는 단순 EDA와 군집분석을 활용하였으나, 이후에는 MCA방법을 사용하였다.

안타깝게도 두 방법 모두 차원축소에 있어서 비교적 고전적 방법에 가까웠다. 보다 좋은 결과물을 도출한 다른 학회원들은 Cat-Boost나 딥러닝의 Embedding Layer를 이용하였다. 이 부분에 대해서는 다른 포스팅에서 자세히 다루기로 하자.


## 네 번째, 알고리즘의 최적화
요즘 머신러닝의 기법들이 매우 빠른 속도로 갱신되고 있기에 사실은 점수를 올리기 위해서는 최신 기법을 서술한 코드를 이해없이 Ctrl+C , Ctrl+V 해주는게 점수를 올리기위한 최적의 전략이라 할수 있다. 그러나 우리팀은 모두가 통계대학원 진학을 생각하고 있고, 또 제대로 이해한 알고리즘을 커스텀해서 써주는 것이 발전을 위해서라도 더 바람직한 길으로 생각했기에 베이지안 옵티마이제이션을 사용하기로 했다.

만약 본선에서 제대로 이해하고 있는 알고리즘을 발표할 수 있었다면, 분명한 메리트가 되었을 것이다. 

> 이상의 포인트가 이번 대회 우리팀의 주요 화두였다. 이 네 가지를 핵심으로 많은 회의를 하고 미팅을 가졌다. 비록 다른 팀에 비해 결과물은 좋지 않았지만 대회를 통해 우리 팀보다 많은 것을 배운 팀은 없으리라 자부한다.


# 발표자료  


# 개선가능성
## 첫번째, 차원축소 방식
최종제출은 하지 않았지만, 우리팀이 내부자료를 이용하여 얻은 Cross-Validation의 결과는 logloss기준의 test스코어가 0.235에 달했다. 리더보드에서 1등을 한 팀이 0.238이 때문에 고무적이었지만, 리더보드에서는 이상하리만큼 0.25에서 줄일 수가 없었다. 

아마 이는 차원축소의 알고리즘이 테스트 데이터에는 언더피팅이 되었으리라고 생각된다. 다음에는 이러한 일이 없게 하기위해 우리팀에서는 민형규가 이 부분을 계속 공부해나갈 것이다.


## 두번째, 최신기술 탐구
전술했듯, 우리가 사용한 기법들은 다소 오래된 것이었다. 이는 우리가 충분히 이해하지 못하는 알고리즘을 쓰는 것에 대한 심적 거부감에 기인한 것으로 보인다. 따라서 앞으로는 최신논문을 계속해서 탐구할 필요가 있어 보인다.

그러나 기존 기법을 버릴 생각은 없다. 이는 우리가 충분히 이해할 수 있을 뿐더러 연구가 충분히 진행되어 있기에 결과물의 해석에 있어서 큰 강점을 가지고 있기 때문이다. 따라서 둘 모두를 병행해서 사용하며 성능은 최신으로, 해석은 구식 기법을 이용하는 것이 적절한 방식으로 보인다.


## 세번째, 체계적 소통
소통의 방식이 덜 체계적이었다. 의사소통이 충분치가 않아서 한 팀원이 겪은 동일한 문제를 다른 팀원도 겪는 경우가 있었다. 이는 각자 역할을 나누어 놓고 다른 팀원들은 그다지 간섭을 하지 않았기 때문으로 보인다. 따라서 다음부터는 이를 방지하기 위해서 현재 진행상황, 에러발생, 해결책, 처리 방법에 대한 체계적인 게시판을 만드려 한다. 이는 비단 다음 대회를 위해서인 것 뿐만 아니라 컴퓨터를 이용한 팀프로젝트 전반에 필요한 습관인 것 같다.


# 결론
아쉬움이 큰 대회다. 정말 좋은 팀원을 만났고, 또 많은 것을 배웠음에도 불구하고 결과물이 좋지 않았기 때문이다.

그럼에도 불구하고 우리팀은 다시 뭉치기로 했다.
다음 대회는 반드시 좋은 성과물을 거둘 것이다.
좌절하지 않고 과정의 중요성을 믿다보면 더 큰 결과물은 반드시 얻을 수 밖에 없을 것이다.

> 다시 한번, 함께 해준 팀원들에게 진심으로 감사의 뜻을 전합니다.
